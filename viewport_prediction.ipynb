{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#360 degree audio-visual content based viewport prediction\n",
    "LSTM-ekf hybrid model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from scipy.io import wavfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Local development paths\n",
    "    DATA_ROOT = Path(\"/home/kirill/Projects/audioVisualVRAttentionModel/data\")\n",
    "    HEAD_DATA_DIR = DATA_ROOT / \"head\"\n",
    "    AUDIO_DIR = DATA_ROOT / \"360videos_with_ambisonic\"      \n",
    "    \n",
    "    DEV_MODE = True\n",
    "    DEV_VIDEO_ID = \"0001\"\n",
    "    \n",
    "    # Audio settings\n",
    "    USE_AUDIO = True\n",
    "    AUDIO_SAMPLE_RATE = 48000  # D-SAV360 ambisonic is 48kHz\n",
    "    AUDIO_FEATURE_DIM = 9  # 1 omnidirectional + 8 directional sectors\n",
    "    VIDEO_FPS = 60\n",
    "    \n",
    "    PREDICTION_HORIZON_SEC = 2.5\n",
    "    INPUT_HISTORY_SEC = 2.0\n",
    "    SAMPLE_RATE_HZ = 90  # Head tracking sample rate\n",
    "    PREDICTION_STEPS = int(PREDICTION_HORIZON_SEC * SAMPLE_RATE_HZ)\n",
    "    INPUT_STEPS = int(INPUT_HISTORY_SEC * SAMPLE_RATE_HZ)\n",
    "    \n",
    "    LSTM_HIDDEN_SIZE = 128\n",
    "    LSTM_NUM_LAYERS = 2\n",
    "    LSTM_DROPOUT = 0.2\n",
    "    \n",
    "    EKF_PROCESS_NOISE = 0.01\n",
    "    EKF_MEASUREMENT_NOISE = 0.001\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_EPOCHS = 50\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    \n",
    "    EVAL_HORIZONS = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
    "    TRAIN_RATIO = 0.7\n",
    "    VAL_RATIO = 0.15\n",
    "\n",
    "config = Config()\n",
    "print(f\"Prediction: {config.INPUT_STEPS} steps -> {config.PREDICTION_STEPS} steps\")\n",
    "print(f\"Audio features: {config.USE_AUDIO} (dim={config.AUDIO_FEATURE_DIM})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spherical Geometry Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalUtils:\n",
    "    @staticmethod\n",
    "    def uv_to_unit_vector(u, v):\n",
    "        theta = u * 2 * np.pi\n",
    "        phi = (v - 0.5) * np.pi\n",
    "        x = np.cos(phi) * np.cos(theta)\n",
    "        y = np.cos(phi) * np.sin(theta)\n",
    "        z = np.sin(phi)\n",
    "        return np.stack([x, y, z], axis=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def unit_vector_to_uv(p):\n",
    "        x, y, z = p[..., 0], p[..., 1], p[..., 2]\n",
    "        theta = np.arctan2(y, x)\n",
    "        theta = np.where(theta < 0, theta + 2 * np.pi, theta)\n",
    "        phi = np.arcsin(np.clip(z, -1, 1))\n",
    "        return theta / (2 * np.pi), phi / np.pi + 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def tangent_velocity(p_t, p_next):\n",
    "        dot = np.sum(p_t * p_next, axis=-1, keepdims=True)\n",
    "        dot = np.clip(dot, -1.0, 1.0)\n",
    "        tangent = p_next - dot * p_t\n",
    "        tangent_norm = np.linalg.norm(tangent, axis=-1, keepdims=True) + 1e-8\n",
    "        angle = np.arccos(dot)\n",
    "        return (tangent / tangent_norm) * angle\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp_map(p, v):\n",
    "        v_norm = np.linalg.norm(v, axis=-1, keepdims=True) + 1e-8\n",
    "        result = np.cos(v_norm) * p + np.sin(v_norm) * (v / v_norm)\n",
    "        return result / (np.linalg.norm(result, axis=-1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(p):\n",
    "        return p / (np.linalg.norm(p, axis=-1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalUtilsTorch:\n",
    "    @staticmethod\n",
    "    def normalize(p):\n",
    "        return p / (torch.norm(p, dim=-1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp_map(p, v):\n",
    "        v_norm = torch.norm(v, dim=-1, keepdim=True) + 1e-8\n",
    "        result = torch.cos(v_norm) * p + torch.sin(v_norm) * (v / v_norm)\n",
    "        return SphericalUtilsTorch.normalize(result)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_loss(p_pred, p_target):\n",
    "        return 1.0 - torch.sum(p_pred * p_target, dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def angular_error_degrees(p_pred, p_target):\n",
    "        dot = torch.clamp(torch.sum(p_pred * p_target, dim=-1), -1.0, 1.0)\n",
    "        return torch.acos(dot) * (180.0 / np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    \"\"\"Extract spatial audio features from D-SAV360 ambisonic WAV files.\n",
    "    \n",
    "    D-SAV360 uses first-order ambisonics (FOA) with 4 channels:\n",
    "    - W: Omnidirectional (pressure)\n",
    "    - X: Front-back axis\n",
    "    - Y: Left-right axis  \n",
    "    - Z: Up-down axis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dir, video_id, num_sectors=8, \n",
    "                 audio_sample_rate=48000, head_sample_rate=90):\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.video_id = video_id\n",
    "        self.num_sectors = num_sectors\n",
    "        self.audio_sample_rate = audio_sample_rate\n",
    "        self.head_sample_rate = head_sample_rate\n",
    "        self.feature_dim = num_sectors + 1  # omnidirectional + sectors\n",
    "        \n",
    "        # Load ambisonic audio\n",
    "        self.audio_data = None\n",
    "        self._load_audio()\n",
    "        \n",
    "    def _load_audio(self):\n",
    "        \"\"\"Load ambisonic WAV file for the video.\"\"\"\n",
    "        # D-SAV360 structure: audio_dir/video_id/video_id.wav\n",
    "        wav_path = self.audio_dir / self.video_id / f\"{self.video_id}.wav\"\n",
    "        \n",
    "        if not wav_path.exists():\n",
    "            print(f\"Warning: Audio file not found at {wav_path}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            sample_rate, audio = wavfile.read(wav_path)\n",
    "            \n",
    "            # Normalize to float [-1, 1]\n",
    "            if audio.dtype == np.int16:\n",
    "                audio = audio.astype(np.float32) / 32768.0\n",
    "            elif audio.dtype == np.int32:\n",
    "                audio = audio.astype(np.float32) / 2147483648.0\n",
    "            \n",
    "            # Expect 4 channels for FOA (W, X, Y, Z)\n",
    "            if len(audio.shape) == 1:\n",
    "                print(f\"Warning: Mono audio, expected 4-channel ambisonics\")\n",
    "                audio = audio.reshape(-1, 1)\n",
    "            \n",
    "            self.audio_data = audio\n",
    "            self.actual_sample_rate = sample_rate\n",
    "            print(f\"Loaded audio: {wav_path.name}, {audio.shape}, {sample_rate}Hz\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio: {e}\")\n",
    "            self.audio_data = None\n",
    "    \n",
    "    def _time_to_audio_sample(self, t_seconds):\n",
    "        \"\"\"Convert head tracking timestamp to audio sample index.\"\"\"\n",
    "        return int(t_seconds * self.actual_sample_rate)\n",
    "    \n",
    "    def _get_directional_energy(self, w, x, y, z, theta, phi):\n",
    "        \"\"\"Compute energy in a specific direction from ambisonics.\n",
    "        \n",
    "        Args:\n",
    "            w, x, y, z: Ambisonic channels (scalars or arrays)\n",
    "            theta: Azimuth angle (0 = front, pi/2 = left)\n",
    "            phi: Elevation angle (0 = horizon, pi/2 = up)\n",
    "        \n",
    "        Returns:\n",
    "            Directional energy estimate\n",
    "        \"\"\"\n",
    "        # Spherical harmonic decoding for first-order ambisonics\n",
    "        # Direction vector\n",
    "        dx = np.cos(phi) * np.cos(theta)\n",
    "        dy = np.cos(phi) * np.sin(theta)\n",
    "        dz = np.sin(phi)\n",
    "        \n",
    "        # Decode: pressure + velocity components weighted by direction\n",
    "        decoded = w + dx * x + dy * y + dz * z\n",
    "        \n",
    "        return np.mean(decoded ** 2)  # Energy\n",
    "    \n",
    "    def extract_features(self, u, v, t_seconds, window_ms=50):\n",
    "        \"\"\"Extract audio features at viewport position and time.\n",
    "        \n",
    "        Args:\n",
    "            u, v: Normalized viewport coordinates (0-1)\n",
    "            t_seconds: Timestamp in seconds\n",
    "            window_ms: Analysis window in milliseconds\n",
    "        \n",
    "        Returns:\n",
    "            Feature vector: [omnidirectional_energy, sector_0, ..., sector_7]\n",
    "        \"\"\"\n",
    "        if self.audio_data is None:\n",
    "            return np.zeros(self.feature_dim, dtype=np.float32)\n",
    "        \n",
    "        # Get audio window\n",
    "        center_sample = self._time_to_audio_sample(t_seconds)\n",
    "        window_samples = int(window_ms / 1000 * self.actual_sample_rate)\n",
    "        start = max(0, center_sample - window_samples // 2)\n",
    "        end = min(len(self.audio_data), center_sample + window_samples // 2)\n",
    "        \n",
    "        if start >= end or end > len(self.audio_data):\n",
    "            return np.zeros(self.feature_dim, dtype=np.float32)\n",
    "        \n",
    "        audio_window = self.audio_data[start:end]\n",
    "        \n",
    "        # Handle different channel counts\n",
    "        if audio_window.shape[1] >= 4:\n",
    "            w, x, y, z = audio_window[:, 0], audio_window[:, 1], audio_window[:, 2], audio_window[:, 3]\n",
    "        elif audio_window.shape[1] == 1:\n",
    "            # Mono fallback\n",
    "            w = audio_window[:, 0]\n",
    "            x = y = z = np.zeros_like(w)\n",
    "        else:\n",
    "            return np.zeros(self.feature_dim, dtype=np.float32)\n",
    "        \n",
    "        # Omnidirectional energy (W channel)\n",
    "        omni_energy = np.mean(w ** 2)\n",
    "        \n",
    "        # Convert viewport (u, v) to spherical angles\n",
    "        viewport_theta = u * 2 * np.pi  # Azimuth\n",
    "        viewport_phi = (v - 0.5) * np.pi  # Elevation\n",
    "        \n",
    "        # Compute energy in sectors around viewport\n",
    "        sector_features = []\n",
    "        for i in range(self.num_sectors):\n",
    "            # Sector direction relative to viewport\n",
    "            sector_angle = 2 * np.pi * i / self.num_sectors\n",
    "            sector_theta = viewport_theta + 0.3 * np.cos(sector_angle)  # ~17° offset\n",
    "            sector_phi = viewport_phi + 0.3 * np.sin(sector_angle)\n",
    "            sector_phi = np.clip(sector_phi, -np.pi/2, np.pi/2)\n",
    "            \n",
    "            energy = self._get_directional_energy(w, x, y, z, sector_theta, sector_phi)\n",
    "            sector_features.append(energy)\n",
    "        \n",
    "        # Normalize features\n",
    "        features = np.array([omni_energy] + sector_features, dtype=np.float32)\n",
    "        max_val = features.max()\n",
    "        if max_val > 0:\n",
    "            features = features / max_val  # Normalize to [0, 1]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_sequence_features(self, u_seq, v_seq, t_seq):\n",
    "        \"\"\"Extract features for a sequence of positions.\"\"\"\n",
    "        features = np.zeros((len(t_seq), self.feature_dim), dtype=np.float32)\n",
    "        for i, (u, v, t) in enumerate(zip(u_seq, v_seq, t_seq)):\n",
    "            features[i] = self.extract_features(u, v, t)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_head_tracking_data(video_id, data_dir):\n",
    "    file_path = data_dir / f\"head_video_{video_id}.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    unit_vecs = SphericalUtils.uv_to_unit_vector(df['u'].values, df['v'].values)\n",
    "    df['px'], df['py'], df['pz'] = unit_vecs[:, 0], unit_vecs[:, 1], unit_vecs[:, 2]\n",
    "    return df\n",
    "\n",
    "def split_by_participant(participant_ids, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    ids = np.array(participant_ids)\n",
    "    np.random.shuffle(ids)\n",
    "    n = len(ids)\n",
    "    n_train, n_val = int(n * train_ratio), int(n * val_ratio)\n",
    "    return ids[:n_train].tolist(), ids[n_train:n_train+n_val].tolist(), ids[n_train+n_val:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewportDataset(Dataset):\n",
    "    def __init__(self, df, participant_ids, input_steps, prediction_steps, \n",
    "                 eval_horizons_steps, audio_extractor=None, use_audio=False):\n",
    "        self.input_steps = input_steps\n",
    "        self.prediction_steps = prediction_steps\n",
    "        self.eval_horizons_steps = eval_horizons_steps\n",
    "        self.use_audio = use_audio and audio_extractor is not None\n",
    "        self.audio_dim = audio_extractor.feature_dim if self.use_audio else 0\n",
    "        self.sequences = []\n",
    "        \n",
    "        df_filtered = df[df['id'].isin(participant_ids)].copy()\n",
    "        \n",
    "        for pid in participant_ids:\n",
    "            pdf = df_filtered[df_filtered['id'] == pid].sort_values('t')\n",
    "            if len(pdf) < input_steps + prediction_steps:\n",
    "                continue\n",
    "                \n",
    "            positions = pdf[['px', 'py', 'pz']].values\n",
    "            timestamps = pdf['t'].values\n",
    "            u_vals = pdf['u'].values\n",
    "            v_vals = pdf['v'].values\n",
    "            \n",
    "            # Compute velocities\n",
    "            velocities = np.zeros_like(positions)\n",
    "            velocities[:-1] = SphericalUtils.tangent_velocity(positions[:-1], positions[1:])\n",
    "            velocities[-1] = velocities[-2]\n",
    "            \n",
    "            # Extract audio features if enabled\n",
    "            if self.use_audio:\n",
    "                audio_features = audio_extractor.extract_sequence_features(\n",
    "                    u_vals, v_vals, timestamps\n",
    "                )\n",
    "            else:\n",
    "                audio_features = None\n",
    "            \n",
    "            total_len = input_steps + prediction_steps\n",
    "            for i in range(len(positions) - total_len + 1):\n",
    "                seq_data = {\n",
    "                    'positions': positions[i:i+total_len],\n",
    "                    'velocities': velocities[i:i+total_len],\n",
    "                    'pid': pid\n",
    "                }\n",
    "                if self.use_audio:\n",
    "                    seq_data['audio'] = audio_features[i:i+total_len]\n",
    "                self.sequences.append(seq_data)\n",
    "        \n",
    "        print(f\"Created {len(self.sequences)} sequences (audio={self.use_audio})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        targets = [seq['positions'][self.input_steps - 1 + h] for h in self.eval_horizons_steps]\n",
    "        \n",
    "        item = {\n",
    "            'input_positions': torch.FloatTensor(seq['positions'][:self.input_steps]),\n",
    "            'input_velocities': torch.FloatTensor(seq['velocities'][:self.input_steps]),\n",
    "            'targets': torch.FloatTensor(np.array(targets))\n",
    "        }\n",
    "        \n",
    "        if self.use_audio:\n",
    "            item['input_audio'] = torch.FloatTensor(seq['audio'][:self.input_steps])\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extended Kalman Filter (Frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalEKF:\n",
    "    def __init__(self, process_noise=0.01, measurement_noise=0.001, dt=1.0/90):\n",
    "        self.dt = dt\n",
    "        self.Q = np.eye(6) * process_noise\n",
    "        self.Q[:3, :3] *= 0.1\n",
    "        self.R = np.eye(3) * measurement_noise\n",
    "        self.F = np.eye(6)\n",
    "        self.F[:3, 3:] = np.eye(3) * dt\n",
    "        self.H = np.zeros((3, 6))\n",
    "        self.H[:3, :3] = np.eye(3)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = np.zeros(6)\n",
    "        self.x[2] = 1.0\n",
    "        self.P = np.eye(6) * 0.1\n",
    "        self.initialized = False\n",
    "    \n",
    "    def predict_trajectory(self, steps):\n",
    "        trajectory = np.zeros((steps, 3))\n",
    "        x_pred = self.x.copy()\n",
    "        for i in range(steps):\n",
    "            x_pred = self.F @ x_pred\n",
    "            x_pred[:3] = SphericalUtils.normalize(x_pred[:3])\n",
    "            trajectory[i] = x_pred[:3]\n",
    "        return trajectory\n",
    "    \n",
    "    def update(self, measurement):\n",
    "        if not self.initialized:\n",
    "            self.x[:3] = measurement\n",
    "            self.initialized = True\n",
    "            return measurement, 0.0\n",
    "        \n",
    "        x_pred = self.F @ self.x\n",
    "        x_pred[:3] = SphericalUtils.normalize(x_pred[:3])\n",
    "        P_pred = self.F @ self.P @ self.F.T + self.Q\n",
    "        y = measurement - self.H @ x_pred\n",
    "        innovation_mag = np.linalg.norm(y)\n",
    "        S = self.H @ P_pred @ self.H.T + self.R\n",
    "        K = P_pred @ self.H.T @ np.linalg.inv(S)\n",
    "        self.x = x_pred + K @ y\n",
    "        self.x[:3] = SphericalUtils.normalize(self.x[:3])\n",
    "        self.P = (np.eye(6) - K @ self.H) @ P_pred\n",
    "        return self.x[:3], innovation_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchEKF:\n",
    "    def __init__(self, config):\n",
    "        self.ekf = SphericalEKF(config.EKF_PROCESS_NOISE, config.EKF_MEASUREMENT_NOISE)\n",
    "    \n",
    "    def process_batch(self, input_positions, eval_horizons_steps):\n",
    "        batch_size, seq_len = input_positions.shape[:2]\n",
    "        positions_np = input_positions.cpu().numpy()\n",
    "        ekf_preds = np.zeros((batch_size, len(eval_horizons_steps), 3))\n",
    "        innovations = np.zeros((batch_size, seq_len))\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            self.ekf.reset()\n",
    "            for t in range(seq_len):\n",
    "                _, innov = self.ekf.update(positions_np[b, t])\n",
    "                innovations[b, t] = innov\n",
    "            traj = self.ekf.predict_trajectory(max(eval_horizons_steps))\n",
    "            for i, h in enumerate(eval_horizons_steps):\n",
    "                ekf_preds[b, i] = traj[h - 1]\n",
    "        \n",
    "        return torch.FloatTensor(ekf_preds).to(input_positions.device), torch.FloatTensor(innovations).to(input_positions.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LSTM Model with Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_size=128, num_layers=2, \n",
    "                 dropout=0.2, num_horizons=5, audio_dim=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        total_input_dim = input_dim + audio_dim\n",
    "        self.audio_dim = audio_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(total_input_dim, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        self.prediction_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size // 2, 3)\n",
    "            ) for _ in range(num_horizons)\n",
    "        ])\n",
    "        \n",
    "        self.gate_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size + 1, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, num_horizons),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_positions, input_velocities, innovation_magnitude, input_audio=None):\n",
    "        # Concatenate trajectory features\n",
    "        lstm_input = torch.cat([input_positions, input_velocities], dim=-1)\n",
    "        \n",
    "        # Add audio if available\n",
    "        if input_audio is not None and self.audio_dim > 0:\n",
    "            lstm_input = torch.cat([lstm_input, input_audio], dim=-1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        hidden = lstm_out[:, -1, :]\n",
    "        \n",
    "        corrections = torch.stack([head(hidden) for head in self.prediction_heads], dim=1)\n",
    "        gate_input = torch.cat([hidden, innovation_magnitude.mean(dim=-1, keepdim=True)], dim=-1)\n",
    "        gates = self.gate_head(gate_input)\n",
    "        \n",
    "        return corrections, gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanLSTMHybrid(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_audio = config.USE_AUDIO\n",
    "        self.eval_horizons_steps = [int(h * config.SAMPLE_RATE_HZ) for h in config.EVAL_HORIZONS]\n",
    "        self.batch_ekf = BatchEKF(config)\n",
    "        \n",
    "        audio_dim = config.AUDIO_FEATURE_DIM if config.USE_AUDIO else 0\n",
    "        self.lstm = SphericalLSTM(\n",
    "            input_dim=6,\n",
    "            hidden_size=config.LSTM_HIDDEN_SIZE,\n",
    "            num_layers=config.LSTM_NUM_LAYERS,\n",
    "            dropout=config.LSTM_DROPOUT,\n",
    "            num_horizons=len(config.EVAL_HORIZONS),\n",
    "            audio_dim=audio_dim\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_positions, input_velocities, input_audio=None):\n",
    "        ekf_preds, innovations = self.batch_ekf.process_batch(\n",
    "            input_positions, self.eval_horizons_steps\n",
    "        )\n",
    "        \n",
    "        corrections, gates = self.lstm(\n",
    "            input_positions, input_velocities, innovations, input_audio\n",
    "        )\n",
    "        \n",
    "        gated_corrections = gates.unsqueeze(-1) * corrections\n",
    "        predictions = SphericalUtilsTorch.exp_map(ekf_preds, gated_corrections)\n",
    "        \n",
    "        return predictions, {\n",
    "            'ekf_predictions': ekf_preds,\n",
    "            'gates': gates,\n",
    "            'innovations': innovations\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loss & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHorizonLoss(nn.Module):\n",
    "    def __init__(self, horizon_weights=None):\n",
    "        super().__init__()\n",
    "        self.horizon_weights = horizon_weights\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        cosine_losses = SphericalUtilsTorch.cosine_loss(predictions, targets)\n",
    "        if self.horizon_weights:\n",
    "            weights = torch.tensor(self.horizon_weights, device=predictions.device)\n",
    "            loss = (cosine_losses * weights.unsqueeze(0)).mean()\n",
    "        else:\n",
    "            loss = cosine_losses.mean()\n",
    "        return loss, {f'loss_h{i}': cosine_losses[:, i].mean().item() for i in range(cosine_losses.shape[1])}\n",
    "\n",
    "def evaluate_model(model, dataloader, config, device):\n",
    "    model.eval()\n",
    "    all_errors = {h: [] for h in config.EVAL_HORIZONS}\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_audio = batch.get('input_audio', None)\n",
    "            if input_audio is not None:\n",
    "                input_audio = input_audio.to(device)\n",
    "            \n",
    "            preds, _ = model(\n",
    "                batch['input_positions'].to(device),\n",
    "                batch['input_velocities'].to(device),\n",
    "                input_audio\n",
    "            )\n",
    "            targets = batch['targets'].to(device)\n",
    "            for i, h in enumerate(config.EVAL_HORIZONS):\n",
    "                errors = SphericalUtilsTorch.angular_error_degrees(preds[:, i], targets[:, i])\n",
    "                all_errors[h].extend(errors.cpu().numpy().tolist())\n",
    "    return {f'MAE_{h}s': np.mean(all_errors[h]) for h in config.EVAL_HORIZONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, num_batches = 0.0, 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_audio = batch.get('input_audio', None)\n",
    "        if input_audio is not None:\n",
    "            input_audio = input_audio.to(device)\n",
    "        \n",
    "        preds, _ = model(\n",
    "            batch['input_positions'].to(device),\n",
    "            batch['input_velocities'].to(device),\n",
    "            input_audio\n",
    "        )\n",
    "        \n",
    "        loss, _ = criterion(preds, batch['targets'].to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return {'train_loss': total_loss / num_batches}\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, num_batches = 0.0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_audio = batch.get('input_audio', None)\n",
    "            if input_audio is not None:\n",
    "                input_audio = input_audio.to(device)\n",
    "            \n",
    "            preds, _ = model(\n",
    "                batch['input_positions'].to(device),\n",
    "                batch['input_velocities'].to(device),\n",
    "                input_audio\n",
    "            )\n",
    "            loss, _ = criterion(preds, batch['targets'].to(device))\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return {'val_loss': total_loss / num_batches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    model = model.to(device)\n",
    "    criterion = MultiHorizonLoss([0.5, 0.7, 0.85, 1.0, 1.0])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss, patience_counter, best_state = float('inf'), 0, None\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_metrics['val_loss'])\n",
    "        \n",
    "        history['train_loss'].append(train_metrics['train_loss'])\n",
    "        history['val_loss'].append(val_metrics['val_loss'])\n",
    "        \n",
    "        if val_metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['val_loss']\n",
    "            patience_counter = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}: train={train_metrics['train_loss']:.4f}, val={val_metrics['val_loss']:.4f}\")\n",
    "        \n",
    "        if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Save/Load & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, config, filepath='viewport_model.pth'):\n",
    "    \"\"\"Save model weights and config for later inference.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': {\n",
    "            'USE_AUDIO': config.USE_AUDIO,\n",
    "            'AUDIO_FEATURE_DIM': config.AUDIO_FEATURE_DIM,\n",
    "            'LSTM_HIDDEN_SIZE': config.LSTM_HIDDEN_SIZE,\n",
    "            'LSTM_NUM_LAYERS': config.LSTM_NUM_LAYERS,\n",
    "            'LSTM_DROPOUT': config.LSTM_DROPOUT,\n",
    "            'EVAL_HORIZONS': config.EVAL_HORIZONS,\n",
    "            'SAMPLE_RATE_HZ': config.SAMPLE_RATE_HZ,\n",
    "            'INPUT_STEPS': config.INPUT_STEPS,\n",
    "            'EKF_PROCESS_NOISE': config.EKF_PROCESS_NOISE,\n",
    "            'EKF_MEASUREMENT_NOISE': config.EKF_MEASUREMENT_NOISE,\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath='viewport_model.pth', device='cpu'):\n",
    "    \"\"\"Load model from checkpoint for inference.\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    cfg = checkpoint['config']\n",
    "    \n",
    "    # Recreate config object\n",
    "    class LoadedConfig:\n",
    "        pass\n",
    "    loaded_config = LoadedConfig()\n",
    "    for k, v in cfg.items():\n",
    "        setattr(loaded_config, k, v)\n",
    "    \n",
    "    # Recreate model\n",
    "    model = KalmanLSTMHybrid(loaded_config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    return model, loaded_config\n",
    "\n",
    "def predict_viewport(model, positions, velocities, audio_features=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Run inference on a single trajectory sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained KalmanLSTMHybrid model\n",
    "        positions: np.array of shape (seq_len, 3) - unit vectors\n",
    "        velocities: np.array of shape (seq_len, 3) - tangent velocities\n",
    "        audio_features: Optional np.array of shape (seq_len, audio_dim)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        predictions: np.array of shape (num_horizons, 3) - predicted unit vectors\n",
    "        metadata: dict with EKF predictions, gates, etc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pos_tensor = torch.FloatTensor(positions).unsqueeze(0).to(device)\n",
    "        vel_tensor = torch.FloatTensor(velocities).unsqueeze(0).to(device)\n",
    "        \n",
    "        audio_tensor = None\n",
    "        if audio_features is not None:\n",
    "            audio_tensor = torch.FloatTensor(audio_features).unsqueeze(0).to(device)\n",
    "        \n",
    "        preds, metadata = model(pos_tensor, vel_tensor, audio_tensor)\n",
    "        \n",
    "        return preds[0].cpu().numpy(), {\n",
    "            'ekf_predictions': metadata['ekf_predictions'][0].cpu().numpy(),\n",
    "            'gates': metadata['gates'][0].cpu().numpy(),\n",
    "        }\n",
    "\n",
    "def predict_to_uv(model, positions, velocities, audio_features=None, device='cpu'):\n",
    "    \"\"\"Run inference and convert predictions to (u, v) coordinates.\"\"\"\n",
    "    preds, metadata = predict_viewport(model, positions, velocities, audio_features, device)\n",
    "    \n",
    "    # Convert unit vectors to UV\n",
    "    u_preds, v_preds = SphericalUtils.unit_vector_to_uv(preds)\n",
    "    \n",
    "    return {\n",
    "        'u': u_preds,\n",
    "        'v': v_preds,\n",
    "        'unit_vectors': preds,\n",
    "        'ekf_predictions': metadata['ekf_predictions'],\n",
    "        'gates': metadata['gates']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Export for Graphs & Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_training_history(history, filepath='training_history.csv'):\n",
    "    \"\"\"Export training history to CSV for external plotting.\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'epoch': range(1, len(history['train_loss']) + 1),\n",
    "        'train_loss': history['train_loss'],\n",
    "        'val_loss': history['val_loss']\n",
    "    })\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Training history exported to {filepath}\")\n",
    "    return df\n",
    "\n",
    "def export_error_metrics(metrics, config, filepath='error_metrics.csv'):\n",
    "    \"\"\"Export error metrics per horizon to CSV.\"\"\"\n",
    "    data = {\n",
    "        'horizon_sec': config.EVAL_HORIZONS,\n",
    "        'mae_degrees': [metrics[f'MAE_{h}s'] for h in config.EVAL_HORIZONS]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Error metrics exported to {filepath}\")\n",
    "    return df\n",
    "\n",
    "def export_detailed_predictions(model, dataloader, config, device, filepath='detailed_predictions.csv'):\n",
    "    \"\"\"Export detailed predictions vs targets for analysis.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= 10:  # Limit to first 10 batches for file size\n",
    "                break\n",
    "                \n",
    "            input_audio = batch.get('input_audio', None)\n",
    "            if input_audio is not None:\n",
    "                input_audio = input_audio.to(device)\n",
    "            \n",
    "            preds, metadata = model(\n",
    "                batch['input_positions'].to(device),\n",
    "                batch['input_velocities'].to(device),\n",
    "                input_audio\n",
    "            )\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            preds_np = preds.cpu().numpy()\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            ekf_preds_np = metadata['ekf_predictions'].cpu().numpy()\n",
    "            gates_np = metadata['gates'].cpu().numpy()\n",
    "            \n",
    "            for i in range(preds_np.shape[0]):\n",
    "                for h_idx, h in enumerate(config.EVAL_HORIZONS):\n",
    "                    # Compute errors\n",
    "                    pred = preds_np[i, h_idx]\n",
    "                    target = targets_np[i, h_idx]\n",
    "                    ekf_pred = ekf_preds_np[i, h_idx]\n",
    "                    \n",
    "                    dot_hybrid = np.clip(np.dot(pred, target), -1, 1)\n",
    "                    dot_ekf = np.clip(np.dot(ekf_pred, target), -1, 1)\n",
    "                    \n",
    "                    error_hybrid = np.degrees(np.arccos(dot_hybrid))\n",
    "                    error_ekf = np.degrees(np.arccos(dot_ekf))\n",
    "                    \n",
    "                    # Convert to UV\n",
    "                    pred_u, pred_v = SphericalUtils.unit_vector_to_uv(pred)\n",
    "                    target_u, target_v = SphericalUtils.unit_vector_to_uv(target)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'batch': batch_idx,\n",
    "                        'sample': i,\n",
    "                        'horizon_sec': h,\n",
    "                        'pred_u': float(pred_u),\n",
    "                        'pred_v': float(pred_v),\n",
    "                        'target_u': float(target_u),\n",
    "                        'target_v': float(target_v),\n",
    "                        'error_hybrid_deg': error_hybrid,\n",
    "                        'error_ekf_deg': error_ekf,\n",
    "                        'gate_value': gates_np[i, h_idx]\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Detailed predictions exported to {filepath} ({len(df)} rows)\")\n",
    "    return df\n",
    "\n",
    "def export_gate_analysis(model, dataloader, config, device, filepath='gate_analysis.csv'):\n",
    "    \"\"\"Export gate values and innovation magnitudes for analysis.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= 20:\n",
    "                break\n",
    "                \n",
    "            input_audio = batch.get('input_audio', None)\n",
    "            if input_audio is not None:\n",
    "                input_audio = input_audio.to(device)\n",
    "            \n",
    "            _, metadata = model(\n",
    "                batch['input_positions'].to(device),\n",
    "                batch['input_velocities'].to(device),\n",
    "                input_audio\n",
    "            )\n",
    "            \n",
    "            gates = metadata['gates'].cpu().numpy()\n",
    "            innovations = metadata['innovations'].cpu().numpy()\n",
    "            \n",
    "            for i in range(gates.shape[0]):\n",
    "                for h_idx, h in enumerate(config.EVAL_HORIZONS):\n",
    "                    results.append({\n",
    "                        'batch': batch_idx,\n",
    "                        'sample': i,\n",
    "                        'horizon_sec': h,\n",
    "                        'gate_value': gates[i, h_idx],\n",
    "                        'mean_innovation': innovations[i].mean(),\n",
    "                        'max_innovation': innovations[i].max()\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Gate analysis exported to {filepath} ({len(df)} rows)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    ax.plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss (Cosine)', fontsize=12)\n",
    "    ax.set_title('Training History', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: training_history.png\")\n",
    "\n",
    "def plot_error_vs_horizon(metrics, config):\n",
    "    \"\"\"Plot angular error vs prediction horizon.\"\"\"\n",
    "    horizons = config.EVAL_HORIZONS\n",
    "    errors = [metrics[f'MAE_{h}s'] for h in horizons]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(horizons, errors, 'o-', lw=2.5, ms=10, color='#2196F3')\n",
    "    ax.fill_between(horizons, 0, errors, alpha=0.2, color='#2196F3')\n",
    "    ax.set_xlabel('Prediction Horizon (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Mean Angular Error (degrees)', fontsize=12)\n",
    "    ax.set_title('Prediction Accuracy vs Horizon', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    for h, e in zip(horizons, errors):\n",
    "        ax.annotate(f'{e:.1f}°', (h, e), textcoords='offset points', \n",
    "                   xytext=(0, 12), ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('error_vs_horizon.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: error_vs_horizon.png\")\n",
    "\n",
    "def plot_gate_distribution(gate_df):\n",
    "    \"\"\"Plot gate value distribution across horizons.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gate values by horizon\n",
    "    ax1 = axes[0]\n",
    "    gate_df.boxplot(column='gate_value', by='horizon_sec', ax=ax1)\n",
    "    ax1.set_xlabel('Prediction Horizon (s)', fontsize=12)\n",
    "    ax1.set_ylabel('Gate Value', fontsize=12)\n",
    "    ax1.set_title('Gate Values by Horizon', fontsize=14)\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # Gate vs Innovation\n",
    "    ax2 = axes[1]\n",
    "    scatter = ax2.scatter(gate_df['mean_innovation'], gate_df['gate_value'], \n",
    "                         c=gate_df['horizon_sec'], cmap='viridis', alpha=0.5, s=20)\n",
    "    ax2.set_xlabel('Mean Innovation Magnitude', fontsize=12)\n",
    "    ax2.set_ylabel('Gate Value', fontsize=12)\n",
    "    ax2.set_title('Gate vs EKF Innovation', fontsize=14)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Horizon (s)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gate_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: gate_analysis.png\")\n",
    "\n",
    "def plot_prediction_scatter(pred_df, horizon=2.5):\n",
    "    \"\"\"Plot predicted vs target positions for a specific horizon.\"\"\"\n",
    "    df_h = pred_df[pred_df['horizon_sec'] == horizon]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # U coordinate\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(df_h['target_u'], df_h['pred_u'], alpha=0.5, s=20)\n",
    "    ax1.plot([0, 1], [0, 1], 'r--', lw=2, label='Perfect')\n",
    "    ax1.set_xlabel('Target U', fontsize=12)\n",
    "    ax1.set_ylabel('Predicted U', fontsize=12)\n",
    "    ax1.set_title(f'U Coordinate @ {horizon}s', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # V coordinate\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(df_h['target_v'], df_h['pred_v'], alpha=0.5, s=20)\n",
    "    ax2.plot([0, 1], [0, 1], 'r--', lw=2, label='Perfect')\n",
    "    ax2.set_xlabel('Target V', fontsize=12)\n",
    "    ax2.set_ylabel('Predicted V', fontsize=12)\n",
    "    ax2.set_title(f'V Coordinate @ {horizon}s', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'prediction_scatter_{horizon}s.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: prediction_scatter_{horizon}s.png\")\n",
    "\n",
    "def plot_ekf_vs_hybrid(pred_df):\n",
    "    \"\"\"Compare EKF-only vs Hybrid model errors.\"\"\"\n",
    "    summary = pred_df.groupby('horizon_sec').agg({\n",
    "        'error_hybrid_deg': 'mean',\n",
    "        'error_ekf_deg': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = summary['horizon_sec']\n",
    "    width = 0.15\n",
    "    \n",
    "    ax.bar(x - width/2, summary['error_ekf_deg'], width, label='EKF Only', color='#FF9800')\n",
    "    ax.bar(x + width/2, summary['error_hybrid_deg'], width, label='Hybrid (EKF+LSTM)', color='#4CAF50')\n",
    "    \n",
    "    ax.set_xlabel('Prediction Horizon (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Mean Angular Error (degrees)', fontsize=12)\n",
    "    ax.set_title('EKF vs Hybrid Model Comparison', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ekf_vs_hybrid.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: ekf_vs_hybrid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (single video for development)\n",
    "print(\"Loading data...\")\n",
    "df = load_head_tracking_data(config.DEV_VIDEO_ID, config.HEAD_DATA_DIR)\n",
    "print(f\"Loaded {len(df)} samples from video {config.DEV_VIDEO_ID}\")\n",
    "\n",
    "# Initialize audio extractor with D-SAV360 ambisonic WAV\n",
    "audio_extractor = None\n",
    "if config.USE_AUDIO:\n",
    "    audio_extractor = AudioFeatureExtractor(\n",
    "        config.AUDIO_DIR, \n",
    "        config.DEV_VIDEO_ID,\n",
    "        audio_sample_rate=config.AUDIO_SAMPLE_RATE,\n",
    "        head_sample_rate=config.SAMPLE_RATE_HZ\n",
    "    )\n",
    "    print(f\"Audio feature dim: {audio_extractor.feature_dim}\")\n",
    "\n",
    "# Split by participant\n",
    "participant_ids = sorted(df['id'].unique().tolist())\n",
    "train_ids, val_ids, test_ids = split_by_participant(participant_ids)\n",
    "print(f\"Participants - Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with audio\n",
    "eval_horizons_steps = [int(h * config.SAMPLE_RATE_HZ) for h in config.EVAL_HORIZONS]\n",
    "\n",
    "train_dataset = ViewportDataset(\n",
    "    df, train_ids, config.INPUT_STEPS, config.PREDICTION_STEPS,\n",
    "    eval_horizons_steps, audio_extractor, config.USE_AUDIO\n",
    ")\n",
    "val_dataset = ViewportDataset(\n",
    "    df, val_ids, config.INPUT_STEPS, config.PREDICTION_STEPS,\n",
    "    eval_horizons_steps, audio_extractor, config.USE_AUDIO\n",
    ")\n",
    "test_dataset = ViewportDataset(\n",
    "    df, test_ids, config.INPUT_STEPS, config.PREDICTION_STEPS,\n",
    "    eval_horizons_steps, audio_extractor, config.USE_AUDIO\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "model = KalmanLSTMHybrid(config)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "model, history = train_model(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights for later inference\n",
    "save_model(model, config, 'viewport_model.pth')\n",
    "\n",
    "# Export training history\n",
    "history_df = export_training_history(history, 'training_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model and export metrics\n",
    "test_metrics = evaluate_model(model, test_loader, config, device)\n",
    "print(\"\\nTest Results:\")\n",
    "for h in config.EVAL_HORIZONS:\n",
    "    print(f\"  MAE @ {h}s: {test_metrics[f'MAE_{h}s']:.2f}°\")\n",
    "\n",
    "# Export error metrics\n",
    "metrics_df = export_error_metrics(test_metrics, config, 'error_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed predictions and gate analysis\n",
    "pred_df = export_detailed_predictions(model, test_loader, config, device, 'detailed_predictions.csv')\n",
    "gate_df = export_gate_analysis(model, test_loader, config, device, 'gate_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all plots\n",
    "plot_training_history(history)\n",
    "plot_error_vs_horizon(test_metrics, config)\n",
    "plot_ekf_vs_hybrid(pred_df)\n",
    "plot_gate_distribution(gate_df)\n",
    "plot_prediction_scatter(pred_df, horizon=2.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
